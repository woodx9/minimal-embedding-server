#!/usr/bin/env python3
"""
OpenAI Compatible Embedding API å‹åŠ›æµ‹è¯•å·¥å…·
æ”¯æŒå¹¶å‘è¯·æ±‚ã€æ‰¹é‡embeddingã€æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
æ”¯æŒ vLLMã€SGLang ç­‰ OpenAI compatible æ¡†æ¶
ä½¿ç”¨ transformers tokenizer ç²¾ç¡®ç”ŸæˆæŒ‡å®š token é•¿åº¦çš„æ–‡æœ¬
æŒç»­å‹åŠ›æµ‹è¯•æ¨¡å¼ - å®æ—¶ç›‘æ§å’Œæ—¥å¿—è®°å½•
"""

import asyncio
import argparse
import time
import random
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
import statistics
import os
import aiohttp
import sys
import json
from transformers import AutoTokenizer


@dataclass
class RequestResult:
    """å•ä¸ªè¯·æ±‚çš„ç»“æœ"""
    success: bool
    latency: float  # æ¯«ç§’
    error_message: str = ""
    timestamp: float = 0.0


@dataclass
class TestMetrics:
    """æµ‹è¯•æŒ‡æ ‡ç»Ÿè®¡"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    latencies: List[float] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    start_time: float = 0.0
    end_time: float = 0.0
    
    # ç”¨äºå®æ—¶ç»Ÿè®¡çš„æ»‘åŠ¨çª—å£ï¼ˆæœ€è¿‘1000ä¸ªè¯·æ±‚ï¼‰
    recent_latencies: List[float] = field(default_factory=list)
    recent_window_size: int = 1000
    
    def add_result(self, result: RequestResult):
        """æ·»åŠ ä¸€ä¸ªè¯·æ±‚ç»“æœ"""
        self.total_requests += 1
        if result.success:
            self.successful_requests += 1
            self.latencies.append(result.latency)
            self.recent_latencies.append(result.latency)
            
            # ä¿æŒæ»‘åŠ¨çª—å£å¤§å°
            if len(self.recent_latencies) > self.recent_window_size:
                self.recent_latencies.pop(0)
        else:
            self.failed_requests += 1
            self.errors.append(result.error_message)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        current_time = time.time()
        total_time = current_time - self.start_time
        
        stats = {
            "æ€»è¯·æ±‚æ•°": self.total_requests,
            "æˆåŠŸè¯·æ±‚æ•°": self.successful_requests,
            "å¤±è´¥è¯·æ±‚æ•°": self.failed_requests,
            "æˆåŠŸç‡": f"{(self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0:.2f}%",
            "è¿è¡Œæ—¶é•¿(ç§’)": f"{total_time:.2f}",
        }
        
        if self.latencies:
            stats.update({
                "å¹³å‡å»¶è¿Ÿ(ms)": f"{statistics.mean(self.latencies):.2f}",
                "ä¸­ä½æ•°å»¶è¿Ÿ(ms)": f"{statistics.median(self.latencies):.2f}",
                "æœ€å°å»¶è¿Ÿ(ms)": f"{min(self.latencies):.2f}",
                "æœ€å¤§å»¶è¿Ÿ(ms)": f"{max(self.latencies):.2f}",
                "æ ‡å‡†å·®(ms)": f"{statistics.stdev(self.latencies) if len(self.latencies) > 1 else 0:.2f}",
            })
            
            # è®¡ç®—ç™¾åˆ†ä½æ•°
            sorted_latencies = sorted(self.latencies)
            p50 = sorted_latencies[int(len(sorted_latencies) * 0.50)]
            p90 = sorted_latencies[int(len(sorted_latencies) * 0.90)]
            p95 = sorted_latencies[int(len(sorted_latencies) * 0.95)]
            p99 = sorted_latencies[int(len(sorted_latencies) * 0.99)]
            
            stats.update({
                "P50å»¶è¿Ÿ(ms)": f"{p50:.2f}",
                "P90å»¶è¿Ÿ(ms)": f"{p90:.2f}",
                "P95å»¶è¿Ÿ(ms)": f"{p95:.2f}",
                "P99å»¶è¿Ÿ(ms)": f"{p99:.2f}",
            })
        
        # è®¡ç®—QPS
        if total_time > 0:
            current_qps = self.total_requests / total_time
            avg_qps = self.successful_requests / total_time
            stats.update({
                "å½“å‰QPS": f"{current_qps:.2f}",
                "å¹³å‡QPS(æˆåŠŸ)": f"{avg_qps:.2f}",
            })
        
        # æœ€è¿‘çª—å£çš„ç»Ÿè®¡ï¼ˆç”¨äºå®æ—¶ç›‘æ§ï¼‰
        if self.recent_latencies:
            stats.update({
                "æœ€è¿‘å¹³å‡å»¶è¿Ÿ(ms)": f"{statistics.mean(self.recent_latencies):.2f}",
                "æœ€è¿‘ä¸­ä½æ•°å»¶è¿Ÿ(ms)": f"{statistics.median(self.recent_latencies):.2f}",
            })
        
        return stats
    
    def get_raw_statistics(self) -> Dict[str, float]:
        """è·å–åŸå§‹æ•°å€¼ç»Ÿè®¡ï¼ˆç”¨äºæ–‡ä»¶è®°å½•ï¼‰"""
        current_time = time.time()
        total_time = current_time - self.start_time
        
        stats = {
            "timestamp": current_time,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "success_rate": (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0,
            "runtime_seconds": total_time,
        }
        
        if self.latencies:
            sorted_latencies = sorted(self.latencies)
            stats.update({
                "avg_latency_ms": statistics.mean(self.latencies),
                "median_latency_ms": statistics.median(self.latencies),
                "min_latency_ms": min(self.latencies),
                "max_latency_ms": max(self.latencies),
                "std_latency_ms": statistics.stdev(self.latencies) if len(self.latencies) > 1 else 0,
                "p50_latency_ms": sorted_latencies[int(len(sorted_latencies) * 0.50)],
                "p90_latency_ms": sorted_latencies[int(len(sorted_latencies) * 0.90)],
                "p95_latency_ms": sorted_latencies[int(len(sorted_latencies) * 0.95)],
                "p99_latency_ms": sorted_latencies[int(len(sorted_latencies) * 0.99)],
            })
        
        if total_time > 0:
            stats.update({
                "current_qps": self.total_requests / total_time,
                "avg_qps_success": self.successful_requests / total_time,
            })
        
        if self.recent_latencies:
            stats.update({
                "recent_avg_latency_ms": statistics.mean(self.recent_latencies),
                "recent_median_latency_ms": statistics.median(self.recent_latencies),
            })
        
        # æ·»åŠ é”™è¯¯ç»Ÿè®¡ä¿¡æ¯
        if self.errors:
            # ç»Ÿè®¡é”™è¯¯ç±»å‹
            error_types = {}
            for error in self.errors:
                error_type = error.split(':')[0] if ':' in error else error[:50]
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            stats.update({
                "error_types": error_types,
                "recent_errors": self.errors[-10:]  # æœ€è¿‘10ä¸ªé”™è¯¯çš„è¯¦ç»†ä¿¡æ¯
            })
        
        return stats


def generate_random_text_with_tokenizer(tokenizer, token_length: int) -> str:
    """
    ä½¿ç”¨ tokenizer ç”Ÿæˆç²¾ç¡® token é•¿åº¦çš„éšæœºæ–‡æœ¬
    
    Args:
        tokenizer: transformers tokenizer
        token_length: ç²¾ç¡®çš„ token é•¿åº¦
        
    Returns:
        str: ç”Ÿæˆçš„æ–‡æœ¬ï¼Œtoken æ•°é‡ç²¾ç¡®ç­‰äº token_length
    """
    # ç”Ÿæˆéšæœºæ•°å­—åºåˆ—ä½œä¸º tokens
    # ä½¿ç”¨æ•°å­—æ˜¯å› ä¸ºå¤§éƒ¨åˆ† tokenizer å¯¹æ•°å­—çš„ tokenization æ¯”è¾ƒç¨³å®š
    random_tokens = [str(random.randint(0, 9999)) for _ in range(token_length * 2)]
    text = ' '.join(random_tokens)
    
    # ç¼–ç å¹¶æˆªæ–­åˆ°ç²¾ç¡®é•¿åº¦
    tokens = tokenizer.encode(text, add_special_tokens=False)
    
    # å¦‚æœ tokens ä¸å¤Ÿï¼Œç»§ç»­æ·»åŠ 
    while len(tokens) < token_length:
        additional_text = ' '.join([str(random.randint(0, 9999)) for _ in range(10)])
        additional_tokens = tokenizer.encode(additional_text, add_special_tokens=False)
        tokens.extend(additional_tokens)
    
    # æˆªæ–­åˆ°ç²¾ç¡®é•¿åº¦
    tokens = tokens[:token_length]
    
    # è§£ç å›æ–‡æœ¬
    text = tokenizer.decode(tokens, skip_special_tokens=True)
    
    # éªŒè¯ token æ•°é‡
    verify_tokens = tokenizer.encode(text, add_special_tokens=False)
    assert len(verify_tokens) == token_length, f"ç”Ÿæˆçš„æ–‡æœ¬ token æ•°é‡ {len(verify_tokens)} ä¸ç­‰äºç›®æ ‡ {token_length}"
    
    return text


class EmbeddingStressTester:
    """Embedding API å‹åŠ›æµ‹è¯•å™¨"""
    
    def __init__(
        self,
        api_key: str,
        base_url: str,
        model: str,
        tokenizer_name: str,
        log_file: str = "stress_test_metrics.jsonl"
    ):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆOpenAI compatibleï¼‰
            base_url: APIåŸºç¡€URL
            model: embeddingæ¨¡å‹åç§°
            tokenizer_name: tokenizer åç§°ï¼ˆç”¨äºç²¾ç¡®ç”Ÿæˆ tokenï¼‰
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')  # ç§»é™¤æœ«å°¾æ–œæ 
        self.model = model
        self.metrics = TestMetrics()
        self.session: Optional[aiohttp.ClientSession] = None
        self.log_file = log_file
        self.log_interval = 2  # æ¯2ç§’è®°å½•ä¸€æ¬¡
        self.display_interval = 1  # æ¯1ç§’æ›´æ–°ä¸€æ¬¡å±å¹•
        self.running = True
        
        # åŠ è½½ tokenizer
        print(f"æ­£åœ¨åŠ è½½ tokenizer: {tokenizer_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
        print(f"âœ“ Tokenizer åŠ è½½æˆåŠŸ\n")
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å–æˆ–åˆ›å»º aiohttp session"""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession()
        return self.session
    
    async def close(self):
        """å…³é—­ session"""
        if self.session and not self.session.closed:
            await self.session.close()
    
    async def single_embedding_request(
        self,
        texts: List[str],
        client_id: int
    ) -> RequestResult:
        """
        æ‰§è¡Œå•ä¸ªembeddingè¯·æ±‚
        
        Args:
            texts: è¦embeddingçš„æ–‡æœ¬åˆ—è¡¨
            client_id: å®¢æˆ·ç«¯ID
            
        Returns:
            RequestResult: è¯·æ±‚ç»“æœ
        """
        start_time = time.time()
        
        try:
            session = await self._get_session()
            
            # æ„é€  OpenAI å…¼å®¹çš„è¯·æ±‚
            url = f"{self.base_url}/v1/embeddings"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            payload = {
                "model": self.model,
                "input": texts
            }
            
            async with session.post(url, json=payload, headers=headers) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"HTTP {response.status}: {error_text}")
                
                # éªŒè¯å“åº”
                result = await response.json()
                if "data" not in result:
                    raise Exception(f"Invalid response format: {result}")
            
            end_time = time.time()
            latency = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            return RequestResult(
                success=True,
                latency=latency,
                timestamp=end_time
            )
            
        except Exception as e:
            end_time = time.time()
            latency = (end_time - start_time) * 1000
            
            error_msg = f"Client {client_id}: {type(e).__name__} - {str(e)}"
            
            return RequestResult(
                success=False,
                latency=latency,
                error_message=error_msg,
                timestamp=end_time
            )
    
    async def client_worker(
        self,
        client_id: int,
        batch_size: int,
        token_length: int,
        continuous: bool = True
    ):
        """
        å•ä¸ªå®¢æˆ·ç«¯å·¥ä½œå™¨ï¼ŒæŒç»­æ‰§è¡Œè¯·æ±‚
        
        Args:
            client_id: å®¢æˆ·ç«¯ID
            batch_size: æ¯ä¸ªè¯·æ±‚çš„batchå¤§å°
            token_length: æ¯ä¸ªæ–‡æœ¬çš„tokené•¿åº¦
            continuous: æ˜¯å¦æŒç»­è¿è¡Œ
        """
        req_num = 0
        while self.running:
            # ä½¿ç”¨ tokenizer ç”Ÿæˆç²¾ç¡® token é•¿åº¦çš„éšæœºæ–‡æœ¬
            texts = [
                generate_random_text_with_tokenizer(self.tokenizer, token_length) 
                for _ in range(batch_size)
            ]
            
            # æ‰§è¡Œè¯·æ±‚
            result = await self.single_embedding_request(texts, client_id)
            
            # è®°å½•ç»“æœ
            self.metrics.add_result(result)
            
            req_num += 1
            
            if not continuous:
                break
    
    async def log_metrics_worker(self):
        """å®šæœŸå°†æŒ‡æ ‡å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        while self.running:
            await asyncio.sleep(self.log_interval)
            
            if self.metrics.total_requests > 0:
                stats = self.metrics.get_raw_statistics()
                stats['datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # å†™å…¥ JSONL æ ¼å¼
                with open(self.log_file, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(stats, ensure_ascii=False) + '\n')
    
    async def display_metrics_worker(self):
        """å®æ—¶æ˜¾ç¤ºæŒ‡æ ‡åˆ°å±å¹•"""
        while self.running:
            await asyncio.sleep(self.display_interval)
            
            if self.metrics.total_requests > 0:
                self.display_live_metrics()
    
    def display_live_metrics(self):
        """åœ¨å±å¹•ä¸Šæ˜¾ç¤ºå®æ—¶æŒ‡æ ‡"""
        # ç§»åŠ¨å…‰æ ‡åˆ°å±å¹•é¡¶éƒ¨å¹¶æ¸…é™¤ä¹‹åçš„å†…å®¹
        # \033[H ç§»åŠ¨åˆ°é¡¶éƒ¨ï¼Œ\033[J æ¸…é™¤å…‰æ ‡ä¹‹åçš„å†…å®¹
        sys.stdout.write("\033[H\033[J")
        sys.stdout.flush()
        
        stats = self.metrics.get_statistics()
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        print(f"{'='*80}")
        print(f"ğŸ”¥ æŒç»­å‹åŠ›æµ‹è¯•å®æ—¶ç›‘æ§ - {current_time}")
        print(f"{'='*80}")
        print(f"ğŸ“‹ æ¨¡å‹: {self.model}")
        print(f"ğŸ“Š æ—¥å¿—æ–‡ä»¶: {self.log_file}")
        print(f"{'='*80}\n")
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ“ˆ è¯·æ±‚ç»Ÿè®¡:")
        print(f"  â”œâ”€ æ€»è¯·æ±‚æ•°:     {stats['æ€»è¯·æ±‚æ•°']:>12,}")
        print(f"  â”œâ”€ æˆåŠŸè¯·æ±‚:     {stats['æˆåŠŸè¯·æ±‚æ•°']:>12,}")
        print(f"  â”œâ”€ å¤±è´¥è¯·æ±‚:     {stats['å¤±è´¥è¯·æ±‚æ•°']:>12,}")
        print(f"  â”œâ”€ æˆåŠŸç‡:       {stats['æˆåŠŸç‡']:>12}")
        print(f"  â””â”€ è¿è¡Œæ—¶é•¿:     {stats['è¿è¡Œæ—¶é•¿(ç§’)']:>12} ç§’\n")
        
        # QPS
        if 'å½“å‰QPS' in stats:
            print(f"ğŸš€ ååé‡:")
            print(f"  â”œâ”€ å½“å‰QPS:      {stats['å½“å‰QPS']:>12}")
            print(f"  â””â”€ å¹³å‡QPS:      {stats['å¹³å‡QPS(æˆåŠŸ)']:>12}\n")
        
        # å»¶è¿Ÿç»Ÿè®¡
        if 'å¹³å‡å»¶è¿Ÿ(ms)' in stats:
            print(f"â±  å»¶è¿Ÿç»Ÿè®¡:")
            print(f"  â”œâ”€ å¹³å‡å»¶è¿Ÿ:     {stats['å¹³å‡å»¶è¿Ÿ(ms)']:>12} ms")
            print(f"  â”œâ”€ ä¸­ä½æ•°å»¶è¿Ÿ:   {stats['ä¸­ä½æ•°å»¶è¿Ÿ(ms)']:>12} ms")
            print(f"  â”œâ”€ æœ€å°å»¶è¿Ÿ:     {stats['æœ€å°å»¶è¿Ÿ(ms)']:>12} ms")
            print(f"  â”œâ”€ æœ€å¤§å»¶è¿Ÿ:     {stats['æœ€å¤§å»¶è¿Ÿ(ms)']:>12} ms")
            print(f"  â””â”€ æ ‡å‡†å·®:       {stats['æ ‡å‡†å·®(ms)']:>12} ms\n")
            
            print(f"ğŸ“Š å»¶è¿Ÿç™¾åˆ†ä½:")
            print(f"  â”œâ”€ P50:          {stats['P50å»¶è¿Ÿ(ms)']:>12} ms")
            print(f"  â”œâ”€ P90:          {stats['P90å»¶è¿Ÿ(ms)']:>12} ms")
            print(f"  â”œâ”€ P95:          {stats['P95å»¶è¿Ÿ(ms)']:>12} ms")
            print(f"  â””â”€ P99:          {stats['P99å»¶è¿Ÿ(ms)']:>12} ms\n")
        
        # æœ€è¿‘çª—å£ç»Ÿè®¡
        if 'æœ€è¿‘å¹³å‡å»¶è¿Ÿ(ms)' in stats:
            print(f"ğŸ”„ æœ€è¿‘{self.metrics.recent_window_size}ä¸ªè¯·æ±‚:")
            print(f"  â”œâ”€ å¹³å‡å»¶è¿Ÿ:     {stats['æœ€è¿‘å¹³å‡å»¶è¿Ÿ(ms)']:>12} ms")
            print(f"  â””â”€ ä¸­ä½æ•°å»¶è¿Ÿ:   {stats['æœ€è¿‘ä¸­ä½æ•°å»¶è¿Ÿ(ms)']:>12} ms\n")
        
        # é”™è¯¯ç»Ÿè®¡
        if self.metrics.failed_requests > 0:
            error_types = {}
            for error in self.metrics.errors[-100:]:  # åªç»Ÿè®¡æœ€è¿‘100ä¸ªé”™è¯¯
                error_type = error.split(':')[0] if ':' in error else error[:50]
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            print(f"âŒ é”™è¯¯ç»Ÿè®¡ (æœ€è¿‘100ä¸ª):")
            for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  â”œâ”€ {error_type}: {count}")
            print()
        
        print(f"{'='*80}")
        print(f"ğŸ’¡ æç¤º: æŒ‰ Ctrl+C åœæ­¢æµ‹è¯•")
        print(f"{'='*80}")
    
    async def run_continuous_stress_test(
        self,
        concurrent_clients: int,
        batch_size: int,
        token_length: int
    ):
        """
        è¿è¡ŒæŒç»­å‹åŠ›æµ‹è¯•
        
        Args:
            concurrent_clients: å¹¶å‘å®¢æˆ·ç«¯æ•°é‡
            batch_size: æ¯ä¸ªè¯·æ±‚çš„batchå¤§å°
            token_length: tokené•¿åº¦
        """
        # æ¸…å±å¹¶æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
        print("\033[2J\033[H", end='', flush=True)
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¯åŠ¨æŒç»­å‹åŠ›æµ‹è¯• - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        print(f"é…ç½®:")
        print(f"  â”œâ”€ å¹¶å‘å®¢æˆ·ç«¯æ•°:   {concurrent_clients}")
        print(f"  â”œâ”€ æ¯è¯·æ±‚Batchæ•°:  {batch_size}")
        print(f"  â”œâ”€ Tokené•¿åº¦:      {token_length}")
        print(f"  â”œâ”€ æ¨¡å‹:           {self.model}")
        print(f"  â”œâ”€ Tokenizer:      {self.tokenizer.name_or_path}")
        print(f"  â”œâ”€ æ—¥å¿—æ–‡ä»¶:       {self.log_file}")
        print(f"  â”œâ”€ æ—¥å¿—é—´éš”:       {self.log_interval}ç§’")
        print(f"  â””â”€ æ˜¾ç¤ºåˆ·æ–°:       {self.display_interval}ç§’")
        print(f"{'='*80}\n")
        print("â³ å‡†å¤‡å¯åŠ¨...", flush=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤´
        with open(self.log_file, 'w', encoding='utf-8') as f:
            header = {
                "start_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "model": self.model,
                "concurrent_clients": concurrent_clients,
                "batch_size": batch_size,
                "token_length": token_length
            }
            f.write(json.dumps(header, ensure_ascii=False) + '\n')
        
        print(f"âœ“ æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {self.log_file}\n")
        await asyncio.sleep(2)
        
        self.metrics.start_time = time.time()
        self.running = True
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        
        # å®¢æˆ·ç«¯å·¥ä½œå™¨
        for i in range(concurrent_clients):
            task = asyncio.create_task(
                self.client_worker(
                    client_id=i,
                    batch_size=batch_size,
                    token_length=token_length,
                    continuous=True
                )
            )
            tasks.append(task)
        
        # æ—¥å¿—è®°å½•å·¥ä½œå™¨
        log_task = asyncio.create_task(self.log_metrics_worker())
        tasks.append(log_task)
        
        # æ˜¾ç¤ºå·¥ä½œå™¨
        display_task = asyncio.create_task(self.display_metrics_worker())
        tasks.append(display_task)
        
        try:
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡ï¼ˆå®é™…ä¸Šä¼šä¸€ç›´è¿è¡Œç›´åˆ°è¢«ä¸­æ–­ï¼‰
            await asyncio.gather(*tasks)
        except asyncio.CancelledError:
            pass
        finally:
            await self.close()
    
    async def run_stress_test(
        self,
        concurrent_clients: int,
        batch_size: int,
        token_length: int,
        requests_per_client: int = 10
    ):
        """
        è¿è¡Œå‹åŠ›æµ‹è¯•ï¼ˆéæŒç»­æ¨¡å¼ï¼Œä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
        
        Args:
            concurrent_clients: å¹¶å‘å®¢æˆ·ç«¯æ•°é‡
            batch_size: æ¯ä¸ªè¯·æ±‚çš„batchå¤§å°
            token_length: tokené•¿åº¦
            requests_per_client: æ¯ä¸ªå®¢æˆ·ç«¯çš„è¯·æ±‚æ•°
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å‹åŠ›æµ‹è¯• - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}")
        print(f"é…ç½®:")
        print(f"  - å¹¶å‘å®¢æˆ·ç«¯æ•°: {concurrent_clients}")
        print(f"  - æ¯è¯·æ±‚Batchæ•°: {batch_size}")
        print(f"  - Tokené•¿åº¦(ç²¾ç¡®): {token_length}")
        print(f"  - æ¯å®¢æˆ·ç«¯è¯·æ±‚æ•°: {requests_per_client}")
        print(f"  - æ€»è¯·æ±‚æ•°: {concurrent_clients * requests_per_client}")
        print(f"  - æ¨¡å‹: {self.model}")
        print(f"  - Tokenizer: {self.tokenizer.name_or_path}")
        print(f"{'='*60}\n")
        
        self.metrics.start_time = time.time()
        self.running = True
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for i in range(concurrent_clients):
            for _ in range(requests_per_client):
                task = asyncio.create_task(
                    self.client_worker(
                        client_id=i,
                        batch_size=batch_size,
                        token_length=token_length,
                        continuous=False
                    )
                )
                tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await asyncio.gather(*tasks)
        
        self.metrics.end_time = time.time()
        
        # å…³é—­ session
        await self.close()
    
    def print_results(self):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"å‹åŠ›æµ‹è¯•ç»“æœ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}\n")
        
        stats = self.metrics.get_statistics()
        
        print("ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"  âœ“ {stats['æ€»è¯·æ±‚æ•°']} ä¸ªè¯·æ±‚")
        print(f"  âœ“ {stats['æˆåŠŸè¯·æ±‚æ•°']} æˆåŠŸ")
        print(f"  âœ— {stats['å¤±è´¥è¯·æ±‚æ•°']} å¤±è´¥")
        print(f"  ğŸ“ˆ æˆåŠŸç‡: {stats['æˆåŠŸç‡']}")
        print(f"  â±  è¿è¡Œæ—¶é•¿: {stats['è¿è¡Œæ—¶é•¿(ç§’)']} ç§’")
        
        if 'å¹³å‡å»¶è¿Ÿ(ms)' in stats:
            print(f"\nâ± å»¶è¿Ÿç»Ÿè®¡:")
            print(f"  â€¢ å¹³å‡å»¶è¿Ÿ: {stats['å¹³å‡å»¶è¿Ÿ(ms)']} ms")
            print(f"  â€¢ ä¸­ä½æ•°å»¶è¿Ÿ: {stats['ä¸­ä½æ•°å»¶è¿Ÿ(ms)']} ms")
            print(f"  â€¢ æœ€å°å»¶è¿Ÿ: {stats['æœ€å°å»¶è¿Ÿ(ms)']} ms")
            print(f"  â€¢ æœ€å¤§å»¶è¿Ÿ: {stats['æœ€å¤§å»¶è¿Ÿ(ms)']} ms")
            print(f"  â€¢ æ ‡å‡†å·®: {stats['æ ‡å‡†å·®(ms)']} ms")
            
            print(f"\nğŸ“Š ç™¾åˆ†ä½æ•°:")
            print(f"  â€¢ P50: {stats['P50å»¶è¿Ÿ(ms)']} ms")
            print(f"  â€¢ P90: {stats['P90å»¶è¿Ÿ(ms)']} ms")
            print(f"  â€¢ P95: {stats['P95å»¶è¿Ÿ(ms)']} ms")
            print(f"  â€¢ P99: {stats['P99å»¶è¿Ÿ(ms)']} ms")
        
        if 'å½“å‰QPS' in stats:
            print(f"\nğŸš€ QPSæŒ‡æ ‡:")
            print(f"  â€¢ å½“å‰QPS: {stats['å½“å‰QPS']}")
            print(f"  â€¢ å¹³å‡QPS(æˆåŠŸ): {stats['å¹³å‡QPS(æˆåŠŸ)']}")
        
        # æ‰“å°é”™è¯¯è¯¦æƒ…
        if self.metrics.failed_requests > 0:
            print(f"\nâŒ å¤±è´¥è¯¦æƒ… (å…± {self.metrics.failed_requests} ä¸ª):")
            # ç»Ÿè®¡é”™è¯¯ç±»å‹
            error_types = {}
            for error in self.metrics.errors:
                error_type = error.split(':')[0] if ':' in error else error
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
                print(f"  â€¢ {error_type}: {count} æ¬¡")
            
            # æ˜¾ç¤ºå‰5ä¸ªè¯¦ç»†é”™è¯¯
            print(f"\n  è¯¦ç»†é”™è¯¯ä¿¡æ¯ (å‰5ä¸ª):")
            for i, error in enumerate(self.metrics.errors[:5], 1):
                print(f"    {i}. {error}")
        
        print(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶: {self.log_file}")
        print(f"\n{'='*80}\n")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='OpenAI Compatible Embedding API å‹åŠ›æµ‹è¯•å·¥å…· (æ”¯æŒ vLLM, SGLang ç­‰)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æŒç»­å‹åŠ›æµ‹è¯•ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
  python stress_test.py --concurrent-clients 10 --batch-size 32 --token-length 512 \\
      --model Qwen/Qwen3-Embedding-0.6B --base-url http://localhost:8000

  # å›ºå®šæ¬¡æ•°æµ‹è¯•
  python stress_test.py --concurrent-clients 10 --batch-size 32 --token-length 512 \\
      --requests-per-client 100 --model Qwen/Qwen3-Embedding-0.6B --base-url http://localhost:8000
        """
    )
    parser.add_argument('--concurrent-clients', type=int, required=True,
                        help='å¹¶å‘å®¢æˆ·ç«¯æ•°é‡')
    parser.add_argument('--batch-size', type=int, required=True,
                        help='æ¯ä¸ªè¯·æ±‚çš„batchå¤§å°')
    parser.add_argument('--token-length', type=int, required=True,
                        help='æ¯ä¸ªæ–‡æœ¬çš„ç²¾ç¡®tokené•¿åº¦')
    parser.add_argument('--requests-per-client', type=int, default=None,
                        help='æ¯ä¸ªå®¢æˆ·ç«¯çš„è¯·æ±‚æ•° (ä¸è®¾ç½®åˆ™æŒç»­è¿è¡Œ)')
    parser.add_argument('--model', type=str, required=True,
                        help='Embeddingæ¨¡å‹åç§° (ä¾‹å¦‚: Qwen/Qwen2.5-1.5B)')
    parser.add_argument('--base-url', type=str, required=True,
                        help='APIåŸºç¡€URL (ä¾‹å¦‚: http://localhost:8000/v1)')
    parser.add_argument('--api-key', type=str, default=None,
                        help='APIå¯†é’¥ (é»˜è®¤: OPENAI_API_KEYç¯å¢ƒå˜é‡)')
    parser.add_argument('--log-file', type=str, default='stress_test_metrics.jsonl',
                        help='æ—¥å¿—æ–‡ä»¶è·¯å¾„ (é»˜è®¤: stress_test_metrics.jsonl)')
    parser.add_argument('--log-interval', type=float, default=2.0,
                        help='æ—¥å¿—è®°å½•é—´éš”(ç§’) (é»˜è®¤: 2.0)')
    parser.add_argument('--display-interval', type=float, default=1.0,
                        help='å±å¹•åˆ·æ–°é—´éš”(ç§’) (é»˜è®¤: 1.0)')
    
    args = parser.parse_args()
    
    # è·å–APIå¯†é’¥
    api_key = args.api_key or os.getenv('OPENAI_API_KEY', 'EMPTY')
    
    # tokenizer ä½¿ç”¨ model åç§°
    tokenizer_name = args.model
    
    # åˆ›å»ºæµ‹è¯•å™¨
    try:
        tester = EmbeddingStressTester(
            api_key=api_key,
            base_url=args.base_url,
            model=args.model,
            tokenizer_name=tokenizer_name,
            log_file=args.log_file
        )
        tester.log_interval = args.log_interval
        tester.display_interval = args.display_interval
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {type(e).__name__} - {str(e)}")
        print(f"\næç¤º: è¯·ç¡®ä¿ tokenizer '{tokenizer_name}' å¯ä»¥æ­£ç¡®åŠ è½½")
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    try:
        if args.requests_per_client is None:
            # æŒç»­å‹åŠ›æµ‹è¯•æ¨¡å¼
            await tester.run_continuous_stress_test(
                concurrent_clients=args.concurrent_clients,
                batch_size=args.batch_size,
                token_length=args.token_length
            )
        else:
            # å›ºå®šæ¬¡æ•°æµ‹è¯•æ¨¡å¼
            await tester.run_stress_test(
                concurrent_clients=args.concurrent_clients,
                batch_size=args.batch_size,
                token_length=args.token_length,
                requests_per_client=args.requests_per_client
            )
            
            # æ‰“å°ç»“æœ
            tester.print_results()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­", flush=True)
        tester.running = False
        await asyncio.sleep(1)  # ç­‰å¾…å·¥ä½œå™¨åœæ­¢
        await tester.close()
        
        # æ¸…å±åæ‰“å°æœ€ç»ˆç»“æœ
        print("\033[2J\033[H", end='', flush=True)
        tester.print_results()
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºç°é”™è¯¯: {type(e).__name__} - {str(e)}")
        tester.running = False
        await tester.close()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
